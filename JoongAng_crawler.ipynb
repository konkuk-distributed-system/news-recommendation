{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f281e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.0020859241485595703 seconds\n"
     ]
    }
   ],
   "source": [
    "## 규원 05.09 중앙일보 클롱링 csv파일 저장 \n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "from tqdm import tqdm  # tqdm 추가\n",
    "import time\n",
    "\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = datetime(2024, 5, 9)\n",
    "\n",
    "\n",
    "## 날짜 형식을 바꿔주는 함수 \n",
    "def date_range(start, end):\n",
    "    for n in range(int((end - start).days) + 1):\n",
    "        yield start + timedelta(n)\n",
    "\n",
    "## 중앙일보 url을 넘겨주면, 해당 url에 있는 기사 list를 가져와주는 함수 \n",
    "def fetch_news_articles(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    articles_url = []\n",
    "    for article in soup.find_all('li', class_='card'):\n",
    "        title_link = article.find('a')\n",
    "        if title_link and title_link.has_attr('href'):\n",
    "            articles_url.append(title_link['href'])\n",
    "            \n",
    "    return articles_url\n",
    "\n",
    "## 요청받은 url에서 url, 제목, 기사내용으로 변형해서 return 해주는 함수  \n",
    "def fetch_articles_with_details(url):\n",
    "    # URL에서 웹 페이지를 요청하고 응답을 받습니다.\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # HTTP 에러가 있을 경우 예외를 발생시킵니다.\n",
    "    \n",
    "    # 응답으로부터 HTML을 파싱합니다.\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 기사 데이터를 저장할 리스트를 초기화합니다.\n",
    "    articles = []\n",
    "    \n",
    "    # 각 기사를 찾아내어 필요한 정보를 추출합니다.\n",
    "    for article in soup.find_all('article'):\n",
    "        # 제목 추출\n",
    "        headline = article.find('h1', class_='headline')\n",
    "        title = headline.get_text(strip=True) if headline else \"No title\"\n",
    "        # 본문 내용 추출\n",
    "        body = article.find('div', class_='article_body fs3')\n",
    "        text = []\n",
    "        if body:\n",
    "            for p in body.find_all('p', attrs={\"data-divno\": True}):\n",
    "                text.append(p.get_text(strip=True))\n",
    "        text_content = ' '.join(text)\n",
    "        \n",
    "        # 기사 URL, 제목, 본문을 튜플로 만들어 리스트에 추가합니다.\n",
    "        print(url+\"진행중\")\n",
    "        articles.append([url, title, text_content])\n",
    "    \n",
    "    return articles\n",
    "\n",
    "\n",
    "# 모든 날짜에 대해 URL을 생성하고 크롤링을 실행, 모든 뉴스 링크 list 전달 \n",
    "def get_news_links():\n",
    "    ## 시작 요일부터 하루씩 올려가며, 데이터 크롤링 \n",
    "    links_url = []\n",
    "    for single_date in date_range(start_date, end_date):\n",
    "        formatted_date = single_date.strftime(\"%Y/%m/%d\")\n",
    "        url = f'https://www.joongang.co.kr/sitemap/index/{formatted_date}'\n",
    "        links = fetch_news_articles(url)\n",
    "        print(f\"Date: {formatted_date}\")\n",
    "        for link in links:\n",
    "            links_url.append(link)\n",
    "    ## 모든 크롤링한 모든 링크의 url 전달 \n",
    "    return links_url\n",
    "\n",
    "\n",
    "## 최종적으로 url 제목 본분 행태로 저장해주는 코드 \n",
    "def joongang_crawler():\n",
    "    crawled_data = []\n",
    "    \n",
    "    ## 모아진 뉴스 링크 가져옴 \n",
    "    links_list = get_news_links()\n",
    "    for link in links_list:\n",
    "        ## 링크에 방문해서 뉴스기사 url 제목 뉴스기사 추출해서 저장\n",
    "        links_detail = fetch_articles_with_details(link)\n",
    "        crawled_data.extend(links_detail) # list에 추가,append로 하면 3차원 배열됨  \n",
    "    #print(crawled_data)\n",
    "    return crawled_data\n",
    "\n",
    "\n",
    "def save_to_csv(data):\n",
    "    # 결과를 저장할 폴더 생성\n",
    "    result_dir = 'result'\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    \n",
    "    # CSV 파일 경로 설정\n",
    "    csv_file_path = os.path.join(result_dir, 'joongang_newsTest.csv')\n",
    "    \n",
    "    # CSV 파일을 열고 데이터를 저장합니다.\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # CSV 파일의 헤더를 작성합니다.\n",
    "        writer.writerow(['URL', 'Title', 'Content'])\n",
    "        # 데이터를 CSV 파일에 작성합니다.\n",
    "        writer.writerows(data)\n",
    "        \n",
    "# 데이터가 예시로 준비되어 있다고 가정하고, 이 함수를 사용하여 저장합니다.\n",
    "example_data = [\n",
    "    ('https://example.com/article1', 'Title 1', 'Content of article 1...'),\n",
    "    ('https://example.com/article2', 'Title 2', 'Content of article 2...'),\n",
    "    ('https://example.com/article3', 'Title 3', 'Content of article 3...'),\n",
    "    ('https://example.com/gitACtionTest', 'Title 3', 'Content of article 3...'),\n",
    "    ('https://example.com/article3', 'Title 3', 'Content of article 3...')\n",
    "]\n",
    "\n",
    "# CSV 파일 저장\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()  # 코드 실행 시작 시간 기록\n",
    "    crawled_data = joongang_crawler()\n",
    "    save_to_csv(example_data)\n",
    "    end_time = time.time()  # 코드 실행 종료 시간 기록\n",
    "    print(f\"Execution time: {end_time - start_time} seconds\")  # 실행 시간 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02b334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2d4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aec2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
